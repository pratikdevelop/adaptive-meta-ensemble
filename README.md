# Adaptive Meta-Ensemble (AME) ğŸš€

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-compatible-orange.svg)](https://scikit-learn.org)

**A novel machine learning algorithm that adaptively weights ensemble members based on input characteristics.**

Unlike traditional ensembles that use fixed weights, AME learns which models work best for different types of data and adjusts their importance dynamically for each prediction.

## ğŸŒŸ Key Features

- **ğŸ¯ Adaptive Weighting**: Models get different importance for different inputs
- **ğŸ§  Meta-Learning**: Learns which models excel in which regions of feature space
- **ğŸ“Š Uncertainty Quantification**: Know when predictions are uncertain
- **ğŸ”§ Auto-Model Selection**: Automatically picks best models from 13 candidates
- **ğŸ“ˆ Proven Performance**: 1-7% accuracy improvements over Random Forest/Gradient Boosting
- **ğŸ” Interpretable**: Visualize which models contribute to each prediction
- **âš¡ Production-Ready**: Drop-in replacement for scikit-learn estimators

## ğŸ“Š Performance Highlights

| Dataset | AME Pro | Random Forest | Gradient Boost | Improvement |
|---------|---------|---------------|----------------|-------------|
| Synthetic Easy | **95.67%** | 88.67% | 88.67% | **+7.0%** |
| Breast Cancer | **97.08%** | 97.08% | 95.91% | **+1.2%** |
| Digits | **99.26%** | 98.52% | 96.30% | **+0.74%** |
| Regression (RÂ²) | **0.9918** | 0.6618 | 0.8308 | **+19.4%** |

## ğŸš€ Quick Start

### Installation

```bash
pip install numpy scikit-learn matplotlib pandas
```

### Basic Usage

```python
from ame_pro import AMEClassifierPro
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and prepare data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features (recommended)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train AME Pro
model = AMEClassifierPro(
    meta_features='advanced',
    meta_learner_type='neural',
    auto_select_models=True,
    n_models_to_keep=5,
    uncertainty_estimation=True,
    verbose=1
)

model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")

# Get predictions with uncertainty
pred, uncertainty = model.predict_with_uncertainty(X_test)

# See which models are most important
importance = model.get_model_importance(X_test)
for model_name, weight in sorted(importance.items(), key=lambda x: x[1], reverse=True):
    print(f"{model_name}: {weight:.4f}")
```

## ğŸ“– How It Works

AME operates in three stages:

1. **Train Base Models**: Trains diverse models (Decision Trees, Random Forests, SVM, etc.)
2. **Learn Meta-Knowledge**: Extracts meta-features from inputs and learns which models work best for different data patterns
3. **Adaptive Prediction**: For each new input, predicts optimal weights for base models and combines their predictions

### Why This Is Novel

Traditional ensembles like:
- **Random Forest**: Uses uniform weights across all trees
- **Gradient Boosting**: Uses fixed sequential weights
- **Voting/Stacking**: Learns fixed weights for all inputs

**AME**: Learns **input-dependent** weights - different models get different importance for each prediction based on that input's characteristics.

## ğŸ›ï¸ Configuration Options

### For Best Accuracy
```python
model = AMEClassifierPro(
    meta_features='advanced',      # Maximum feature extraction
    meta_learner_type='neural',    # Most powerful
    n_models_to_keep=7,           # More models
    uncertainty_estimation=True
)
```

### For Speed
```python
model = AMEClassifierPro(
    meta_features='statistical',   # Basic features
    meta_learner_type='tree',      # Faster
    n_models_to_keep=3            # Fewer models
)
```

### Custom Models
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

custom_models = [
    ('rf', RandomForestClassifier(n_estimators=200)),
    ('gb', GradientBoostingClassifier(n_estimators=200))
]

model = AMEClassifierPro(base_models=custom_models)
```

## ğŸ“š Documentation

- **[Quick Start Guide](QUICK_START.md)** - Get running in 5 minutes
- **[Research Paper](AME_research_paper.md)** - Technical details and methodology
- **[Publication Guide](PUBLICATION_GUIDE.md)** - Academic publication roadmap
- **[Executive Summary](EXECUTIVE_SUMMARY.md)** - Business and commercialization strategy

## ğŸ§ª Running Examples

```bash
# Run basic demo
python demo_ame.py

# Run comprehensive benchmarks
python comprehensive_benchmark.py
```

## ğŸ”¬ Algorithm Details

### Meta-Features Extracted

**Statistical**: Mean, std, min, max, median, percentiles  
**Complexity**: Range, sparsity, variance, entropy  
**Advanced**: Skewness, kurtosis, coefficient of variation

### Base Models (Auto-Selected)

**Classification**: Decision Tree, Random Forest, Extra Trees, Gradient Boosting, Logistic Regression, SVM, KNN, Naive Bayes, MLP

**Regression**: Decision Tree, Random Forest, Extra Trees, Gradient Boosting, Ridge, Lasso, ElasticNet, SVR, KNN, MLP

### Meta-Learners

- **Neural Network**: Most accurate, learns complex patterns
- **Random Forest**: Robust, good generalization
- **Decision Tree**: Fast, interpretable

## ğŸ¯ Use Cases

### âœ… Perfect For
- Complex, heterogeneous datasets
- High-stakes decisions needing uncertainty estimates
- When different patterns exist in different data regions
- Interpretable ensemble needed
- Have 1000+ training samples

### âš ï¸ Consider Alternatives For
- Simple, homogeneous datasets â†’ Use Random Forest
- Very small datasets (<500 samples) â†’ Use simpler models
- Images/text/audio â†’ Use Deep Learning
- Need extreme speed â†’ Use Gradient Boosting

## ğŸ“ˆ Benchmark Results

Full benchmark results available in:
- `classification_benchmarks.csv` - All classification results
- `regression_benchmarks.csv` - All regression results
- `benchmark_comparison.png` - Visual comparisons

Tested against:
- Random Forest
- Gradient Boosting
- Voting Ensemble
- Stacking Ensemble

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Additional meta-feature extraction methods
- [ ] Support for multi-output tasks
- [ ] Online/incremental learning
- [ ] GPU acceleration
- [ ] More comprehensive tests
- [ ] Additional examples and tutorials

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“– Citation

If you use AME in your research, please cite:

```bibtex
@software{ame2026,
  title={Adaptive Meta-Ensemble: Input-Dependent Model Weighting for Machine Learning},
  author={Pratik Kumar},
  year={2026},
  url={https://github.com/pratikdevelop/adaptive-meta-ensemble}
}
```

## ğŸ™ Acknowledgments

Built using scikit-learn's excellent machine learning framework. Inspired by research in ensemble learning, meta-learning, and adaptive systems.

## ğŸ“ Contact & Support

- **GitHub Issues**: For bugs and feature requests
- **Discussions**: For questions and general discussion

## ğŸ—ºï¸ Roadmap

### v1.1 (Next Release)
- [ ] PyPI package distribution
- [ ] Enhanced documentation site
- [ ] More example notebooks
- [ ] Performance optimizations

### v2.0 (Future)
- [ ] Deep learning base models
- [ ] Automated hyperparameter tuning
- [ ] Distributed training support
- [ ] REST API for serving

## â­ Star History

If you find AME useful, please consider starring the repository!

---

**Made with â¤ï¸ by [Pratik Raut](https://github.com/pratikdevelop)**

**Ready to improve your ML models? Try AME today!** ğŸš€